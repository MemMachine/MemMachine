---
title: "Using Ollama Models"
description: "Guide to configure MemMachine to use Ollama models."
icon: "paw"
---

## Prerequisites

Before you begin the installation and configuration of MemMachine, you must ensure that your local environment is ready by having Ollama installed and the necessary models downloaded.



### 1. Ollama Service

MemMachine connects directly to **Ollama** using its local API, which must be running in the background.

- **Installation:** If you do not yet have Ollama installed, please follow the official setup guide.

  - **Resource:** [Download Ollama](https://ollama.com/download)

- **Start the Service:** Once installed, start the Ollama service to make the local API available. Open your terminal or command prompt and run:

  ```bash
  ollama serve
  ```

- **Verification:** You can confirm the service is running successfully by opening your web browser and navigating to the following address: `http://localhost:11434`
    You should see the Ollama web interface if the service is active.

### 2. Required Ollama Models

MemMachine requires **two** types of models to function: a Large Language Model (LLM) for generative tasks and an embedding model for converting text into vectors (e.g., for retrieval-augmented generation).

You must download these models to your local Ollama repository **before** starting MemMachine.

- **Download Models:** Use the `ollama pull` command to download the models you want.

| **Model Type**                 | **Example Model ID** | **Command to Run**             |
| ------------------------------ | -------------------- | ------------------------------ |
| **Large Language Model (LLM)** | Llama 3              | `ollama pull llama3`           |
| **Embedding Model**            | Nomic Embed Text     | `ollama pull nomic-embed-text` |

<Note> You can choose any compatible LLM (like `mixtral`, `gemma`, etc.) and embedding model available on Ollama, but the examples above are recommended starting points.</Note>

- **View Downloaded Models:** To see a list of all models currently available in your local Ollama repository, run:

  ```bash
  ollama list
  ```

## Installation: QuickStart Configuration

The installation script will automatically guide you through setting up your **Large Language Model (LLM) provider**. When prompted, you **must** select **Ollama** to integrate with **Ollama**.

Your prompt input should match the following example:

```bash
[PROMPT] Which provider would you like to use? (OpenAI/Bedrock/Ollama) [OpenAI]: Ollama
[INFO] Selected provider: OLLAMA
```

Ollama Configuration and Model Choices
You’ll then be prompted to select:

	•	Ollama base URL (default: http://host.docker.internal:11434/v1)

	•	Choice of LLM (Large Language Model)

            example: llama3

	•	Choice of Embedding Model

            example: nomic-embed-text

<Note> If you are unsure about model selection, simply press **Enter** at the respective prompts to use the recommended default options.</Note>

Congratulations!  You have now successfully deployed MemMachine using AWS Bedrock!

## Manually Configuring MemMachine to use Ollama

If you already have MemMachine installed and wish to switch to Ollama manually, you can do so by updating the configuration file, `./cfg.yml`.

<Note> Within the 'cfg.yml' file, make sure duplicate models are removed.  IE, if you are using the ollama_model, ensure that any other model configurations (like openai_model) are commented out or deleted to avoid conflicts. </Note>

Below is an example configuration snippets for setting up AWS Bedrock as different components of MemMachine:

### LLM Provider

To set your LLM Model for Bedrock, you will want to use the following fields:

| Parameter                     | Required? | Default           | Description                                                                                    |
| ----------------------------- | --------- | ----------------- | ---------------------------------------------------------------------------------------------- |
| `Model:`                      | Yes       | N/A               | The Model block in the configuration                                                           |
| `ollama_model:`               | Yes       | N/A               | Tag you can use if setting AWS Bedrock for your Embedder.                                      |
| `model_vendor`                | Yes       | N/A               | The vendor of the model (e.g., `openai-compatible`).                                           |
| `model`                       | Yes       | N/A               | The name of the model to use (e.g., `llama3`).                                                 |
| `api_key`                     | Yes       | N/A               | The API key field must exist, but needs to be set to `EMPTY`                                   |
| `base_url`                    | Yes       | N/A               | The base URL for accessing your ollama service (e.g., `http://host.docker.internal:11434/v1`). |

Here's an example of what the Ollama LLM Model configuration would look like in `cfg.yml`:

```yaml
Model:
  ollama_model:
    model_vendor: openai-compatible
    model: "llama3"
    api_key: "EMPTY"
    base_url: "http://host.docker.internal:11434/v1"
    ```

### Embedder Configuration

To set your Embedding Model for Ollama, you will want to use the following fields:
| Parameter          | Required?  | Default          | Description                                                                           |
| ------------------ | ---------- | --------------- | ------------------------------------------------------------------------------------- |
| `embedder:`        | Yes        | N/A             | The embedder block in the configuration.                                              |
| `ollama_embedder:` | Yes        | N/A             | Tag you can use if setting AWS Bedrock for your Embedder.                             |
| `name:`            | Yes        | N/A             | The name of the API client to use. For Ollama's local API compatibility, this must be set to `openai`. |
| `config:`          | Yes        | N/A             | Designates the config sub-block in the embedder block for the confinguration file. |
| `model:`           | Yes        | N/A             | The Ollama **embedding model ID** to use for generating vectors. This model must be downloaded and running locally on your Ollama service. |
| `api_key`          | Yes        | N/A             | The API key placeholder. Since Ollama's local API typically **doesn't require a key**, it is common practice to set this to `"EMPTY"` or any placeholder string. |
| `base_url`         | Yes        | N/A             | The **URL of the Ollama API endpoint**. The format is usually `http://<host>:<port>/v1`.  The host `host.docker.internal` is used when running a configuration *inside* a Docker container to reach a service (Ollama) running on the *host machine*. |
| `dimensions`       | Yes        | N/A             | The expected output dimension of the vector embeddings. This value **must match** the dimension size of the specific embedding model (e.g., `nomic-embed-text` is 768). |

Here's an example of what the Ollama embedder configuration would look like in `cfg.yml`:
```yaml
embedder:
    ollama_embedder:
        name: openai
        config:
            model: "nomic-embed-text"
            api_key: "EMPTY"
            base_url: "http://host.docker.internal:11434/v1"
            dimensions: 768
    ```

With these configurations in place, MemMachine will be set up to use Ollama for both its LLM and embedding functionalities. 
Make sure to restart MemMachine after making these changes to apply the new settings. 

*Need Help?*  Refer to the [Install Guide](./install_guide) for how to start and test your configuration with MemMachine.